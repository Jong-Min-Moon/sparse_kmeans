#!/bin/bash

# Exit immediately if a command exits with a non-zero status.
set -e

# --- Configuration Variables ---
# Base directory for experiments, R scripts, job files, and output files
TABLE_NAME="R_mnist" # A distinct table name for R results
BASE_DIR="/home1/jongminm/sparse_kmeans/experiment/R_simulations/29_08_11_2025/${TABLE_NAME}"
# Path to the SQLite database (for potential future use if results are inserted by bash)
DB_DIR="/home1/jongminm/sparse_kmeans/sparse_kmeans.db"

# --- Simulation Parameters ---
# These variables will define the loops for R simulations
# DIMENSION will be the 'dimension' parameter for the R script (corresponds to N in your original bash loop)
# REP_NUM will be the 'rep_num' parameter for the R script (corresponds to REP in your original bash loop)

# --- Ensure Base Directory Exists ---
# Create the base directory if it doesn't already exist
mkdir -p "$BASE_DIR"

echo "Starting SLURM job generation for R simulations..."
echo "Base Directory: $BASE_DIR"
echo "Database Directory: $DB_DIR (for potential future use)"
echo "Table Name: $TABLE_NAME"

# --- Loop through simulation parameters ---
# Loop for 'REP_NUM' (main repetition number) from 1 to 50
for REP_NUM in $(seq 1 50); do
    # Loop for 'DIMENSION' (number of features/dimensions for the R script)
    for DIMENSION in 40000 60000 80000 100000; do

        # Define filenames based on current parameters
        RFILE_NAME="${TABLE_NAME}_dim${DIMENSION}_rep${REP_NUM}" # Name without .R extension
        RFILE="$BASE_DIR/${RFILE_NAME}.R"
        JOBFILE="$BASE_DIR/${RFILE_NAME}.sh"
        OUTFILE="$BASE_DIR/${RFILE_NAME}.out"

        echo "Generating files for dimension=${DIMENSION}, rep=${REP_NUM}..."

        # === Create R .R file ===
        # This block writes the R code into the .R file
        cat > "$RFILE" <<EOF
# R script generated by SLURM job generator

# Get parameters from command line arguments
args = commandArgs(trailingOnly=TRUE)
dimension = as.numeric(args[1]) # Corresponds to DIMENSION from bash loop
rep_num = as.numeric(args[2])   # Corresponds to REP_NUM from bash loop

# Load libraries and source functions
# IMPORTANT: Ensure these paths are correct and accessible from the compute node.
# You might need to adjust "D:/GitHub/sparse_kmeans/..." to a path accessible on your cluster.
library(sparcl)
library(RSQLite) # Included if you plan to insert into SQLite from R
source("D:/GitHub/sparse_kmeans/experiment/25_04_03_2025/competitor.R")
source("D:/GitHub/sparse_kmeans/experiment/25_04_03_2025/data_generation.R")
source("D:/GitHub/sparse_kmeans/experiment/25_04_03_2025/get_proportional_subsample")

# Load full dataset (assuming these paths are fixed and accessible from the compute node)
# IMPORTANT: Ensure these CSV files are available on the compute nodes at this exact path.
# If not, you'll need to adjust the paths or copy the files to a shared file system.
x_full <- read.csv("D:/GitHub/sparse_kmeans/experiment/28_07_30_2025/real/selected_data_small.csv")
y_full <- read.csv("D:/GitHub/sparse_kmeans/experiment/28_07_30_2025/real/selected_labels_small.csv")

# Initialize sums for accuracy and timing
acc_witten_sum = 0
time_witten_sum = 0
acc_arias_sum = 0
time_arias_sum = 0

# Loop for sub-repetitions (30 times as in your original R script)
for (i in 1:30){
    # Use dimension (from bash) and i (sub-rep) for data generation/subsampling
    # The '45' in your original R code is now 'dimension' from the bash loop
    data = get_proportional_subsample(x_full, y_full, dimension, i)

    # Witten's method (sparse_kmeans_ell1_ell2)
    start_time_witten = Sys.time()
    sol_witten = sparse_kmeans_ell1_ell2(data$X)
    end_time_witten = Sys.time()
    time_witten_sum = time_witten_sum + as.numeric(end_time_witten - start_time_witten, units="secs")
    acc_witten_sum = acc_witten_sum + evaluate_clustering(sol_witten, data$y)

    # Arias' method (sparse_kmeans_hillclimb)
    start_time_arias = Sys.time()
    sol_arias = sparse_kmeans_hillclimb(data$X)
    end_time_arias = Sys.time()
    time_arias_sum = time_arias_sum + as.numeric(end_time_arias - start_time_arias, units="secs")
    acc_arias_sum = acc_arias_sum + evaluate_clustering(sol_arias, data$y)
}

# Calculate averages over the 30 sub-repetitions
avg_acc_witten = acc_witten_sum / 30
avg_time_witten = time_witten_sum / 30
avg_acc_arias = acc_arias_sum / 30
avg_time_arias = time_arias_sum / 30

# Print results in a key=value format for the bash script to capture
cat(paste0("avg_acc_witten=", avg_acc_witten, "\n"))
cat(paste0("avg_time_witten=", avg_time_witten, "\n"))
cat(paste0("avg_acc_arias=", avg_acc_arias, "\n"))
cat(paste0("avg_time_arias=", avg_time_arias, "\n"))

# Exit R cleanly after script execution
q("no")
EOF

        # === Create SLURM Job Script (.sh file) ===
        # This block writes the SLURM batch script
        cat > "$JOBFILE" <<EOF
#!/bin/bash
#SBATCH --job-name=R_${TABLE_NAME}_dim${DIMENSION}_rep${REP_NUM} # Job name for easier identification
#SBATCH --output="${OUTFILE}"             # Standard output and error log file
#SBATCH --partition=main                  # Specify the partition to use (e.g., 'main', 'debug')
#SBATCH --nodes=1                         # Request 1 node
#SBATCH --ntasks=1                        # Request 1 task (process)
#SBATCH --cpus-per-task=4                 # Request 4 CPUs per task (adjust based on R's needs)
#SBATCH --mem=8G                          # Request 8 GB of memory (adjust based on data size)
#SBATCH --time=1:59:59                    # Set maximum job run time (HH:MM:SS)

# Echo start time and hostname for logging
echo "Starting R job for dimension=${DIMENSION}, rep=${REP_NUM} on \$(hostname) at \$(date)"

# Load necessary modules
# IMPORTANT: Verify these module commands are correct for your cluster environment.
module purge                               # Unload all currently loaded modules
module load legacy/CentOS7                 # Load specific OS environment if required
module load R/4.2.0                        # Load the R module (adjust version if needed)

# Change to the base directory where the R script is located
# Use '|| { ... }' for robust error handling if directory change fails
cd "$BASE_DIR" || { echo "Error: Could not change to directory $BASE_DIR. Exiting."; exit 1; }

# Run the R script in batch mode
# Pass DIMENSION and REP_NUM as command-line arguments to the R script
Rscript "${RFILE_NAME}.R" "${DIMENSION}" "${REP_NUM}"

# Echo end time for logging
echo "Finished R job for dimension=${DIMENSION}, rep=${REP_NUM} at \$(date)"
EOF

        # === Submit SLURM Job ===
        # Submit the generated job script to the SLURM scheduler
        echo "Submitting job: $JOBFILE"
        sbatch "$JOBFILE"

        # Pause for 1 second to avoid overwhelming the SLURM scheduler
        sleep 1

    done # End of DIMENSION loop
done     # End of REP_NUM loop

echo "All SLURM R jobs have been generated and submitted."