
@article{cai_high_2019,
	title = {High {Dimensional} {Linear} {Discriminant} {Analysis}: {Optimality}, {Adaptive} {Algorithm} and {Missing} {Data}},
	volume = {81},
	issn = {1369-7412},
	shorttitle = {High {Dimensional} {Linear} {Discriminant} {Analysis}},
	url = {https://doi.org/10.1111/rssb.12326},
	doi = {10.1111/rssb.12326},
	abstract = {The paper develops optimality theory for linear discriminant analysis in the high dimensional setting. A data-driven and tuning-free classification rule, which is based on an adaptive constrained l1-minimization approach, is proposed and analysed. Minimax lower bounds are obtained and this classification rule is shown to be simultaneously rate optimal over a collection of parameter spaces. In addition, we consider classification with incomplete data under the missingness completely at random model. An adaptive classifier with theoretical guarantees is introduced and the optimal rate of convergence for high dimensional linear discriminant analysis under the missingness completely at random model is established. The technical analysis for the case of missing data is much more challenging than that for complete data. We establish a large deviation result for the generalized sample covariance matrix, which serves as a key technical tool and can be of independent interest. An application to lung cancer and leukaemia studies is also discussed.},
	number = {4},
	urldate = {2024-02-03},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Cai, T. Tony  and Zhang, Linjun},
	month = sep,
	year = {2019},
	pages = {675--705},
	file = {Tony Cai_Zhang_2019_High Dimensional Linear Discriminant Analysis.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Tony Cai_Zhang_2019_High Dimensional Linear Discriminant Analysis.pdf:application/pdf},
}


@article{cai_constrained_2011,
	title = {A {Constrained} $\ell_1$ {Minimization} {Approach} to {Sparse} {Precision} {Matrix} {Estimation}},
	volume = {106},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/jasa.2011.tm10155},
	doi = {10.1198/jasa.2011.tm10155},
	abstract = {This article proposes a constrained ℓ1 minimization method for estimating a sparse inverse covariance matrix based on a sample of n iid p-variate random variables. The resulting estimator is shown to have a number of desirable properties. In particular, the rate of convergence between the estimator and the true s-sparse precision matrix under the spectral norm is when the population distribution has either exponential-type tails or polynomial-type tails. We present convergence rates under the elementwise ℓ∞ norm and Frobenius norm. In addition, we consider graphical model selection. The procedure is easily implemented by linear programming. Numerical performance of the estimator is investigated using both simulated and real data. In particular, the procedure is applied to analyze a breast cancer dataset and is found to perform favorably compared with existing methods.},
	number = {494},
	urldate = {2024-01-26},
	journal = {Journal of the American Statistical Association},
	author = {Cai, Tony and Liu, Weidong and Luo, Xi},
	month = jun,
	year = {2011},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/jasa.2011.tm10155},
	keywords = {Covariance matrix, Frobenius norm, Gaussian graphical model, Precision matrix, Rate of convergence, Spectral norm},
	pages = {594--607},
	file = {A Constrained  1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:C\:\\Users\\Jongmin\\Zotero\\storage\\WVEZ2MWW\\A Constrained  1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:application/pdf;Cai et al_2011_A Constrained ℓ1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:G\:\\My Drive\\paper\\Cai et al_2011_A Constrained ℓ1 Minimization Approach to Sparse Precision Matrix Estimation.pdf:application/pdf},
}

@article{fan_innovated_2016,
	title = {Innovated scalable efficient estimation in ultra-large {Gaussian} graphical models},
	volume = {44},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-5/Innovated-scalable-efficient-estimation-in-ultra-large-Gaussian-graphical-models/10.1214/15-AOS1416.full},
	doi = {10.1214/15-AOS1416},
	abstract = {Large-scale precision matrix estimation is of fundamental importance yet challenging in many contemporary applications for recovering Gaussian graphical models. In this paper, we suggest a new approach of innovated scalable efficient estimation (ISEE) for estimating large precision matrix. Motivated by the innovated transformation, we convert the original problem into that of large covariance matrix estimation. The suggested method combines the strengths of recent advances in high-dimensional sparse modeling and large covariance matrix estimation. Compared to existing approaches, our method is scalable and can deal with much larger precision matrices with simple tuning. Under mild regularity conditions, we establish that this procedure can recover the underlying graphical structure with significant probability and provide efficient estimation of link strengths. Both computational and theoretical advantages of the procedure are evidenced through simulation and real data examples.},
	number = {5},
	urldate = {2024-01-26},
	journal = {The Annals of Statistics},
	author = {Fan, Yingying and Lv, Jinchi},
	month = oct,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F12, 62H12, 62J05, big data, efficiency, Gaussian graphical model, precision matrix, scalability, Sparsity},
	pages = {2098--2126},
	file = {Fan_Lv_2016_Innovated scalable efficient estimation in ultra-large Gaussian graphical models.pdf:G\:\\My Drive\\paper\\Fan_Lv_2016_Innovated scalable efficient estimation in ultra-large Gaussian graphical models.pdf:application/pdf},
}

@article{fan_optimal_2013,
	title = {Optimal {Classification} in {Sparse} {Gaussian} {Graphic} {Model}},
	volume = {41},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/23566558},
	abstract = {Consider a two-class classification problem where the number of features is much larger than the sample size. The features are masked by Gaussian noise with mean zero and covariance matrix Σ, where the precision matrix Ω = Σ -1 is unknown but is presumably sparse. The useful features, also unknown, are sparse and each contributes weakly (i.e., rare and weak) to the classification decision. By obtaining a reasonably good estimate of Ω, we formulate the setting as a linear regression model. We propose a two-stage classification method where we first select features by the method of Innovated Thresholding (IT), and then use the retained features and Fisher's LDA for classification. In this approach, a crucial problem is how to set the threshold of IT. We approach this problem by adapting the recent innovation of Higher Criticism Thresholding (HCT). We find that when useful features are rare and weak, the limiting behavior of HCT is essentially just as good as the limiting behavior of ideal threshold, the threshold one would choose if the underlying distribution of the signals is known (if only). Somewhat surprisingly, when Ω is sufficiently sparse, its off-diagonal coordinates usually do not have a major influence over the classification decision. Compared to recent work in the case where Ω is the identity matrix [Proc. Natl. Acad. Sci. USA 105 (2008) 14790—14795; Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng. Sci. 367 (2009) 4449—4470], the current setting is much more general, which needs a new approach and much more sophisticated analysis. One key component of the analysis is the intimate relationship between HCT and Fisher's separation. Another key component is the tight large-deviation bounds for empirical processes for data with unconventional correlation structures, where graph theory on vertex coloring plays an important role.},
	number = {5},
	urldate = {2024-01-26},
	journal = {The Annals of Statistics},
	author = {Fan, Yingying and Jin, Jiashun and Yao, Zhigang},
	year = {2013},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2537--2571},
	file = {Fan et al_2013_Optimal Classification in Sparse Gaussian Graphic Model.pdf:G\:\\My Drive\\paper\\Fan et al_2013_Optimal Classification in Sparse Gaussian Graphic Model.pdf:application/pdf},
}

@misc{zhuang_statistically_2023,
	title = {Statistically {Optimal} {K}-means {Clustering} via {Nonnegative} {Low}-rank {Semidefinite} {Programming}},
	url = {http://arxiv.org/abs/2305.18436},
	doi = {10.48550/arXiv.2305.18436},
	abstract = {\$K\$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the \$K\$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed \$K\$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-the-art.},
	urldate = {2024-01-24},
	publisher = {arXiv},
	author = {Zhuang, Yubo and Chen, Xiaohui and Yang, Yun and Zhang, Richard Y.},
	month = sep,
	year = {2023},
	note = {arXiv:2305.18436 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Zhuang et al_2023_Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Zhuang et al_2023_Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite.pdf:application/pdf},
}

@article{chen_hansonwright_2021,
	title = {Hanson–{Wright} inequality in {Hilbert} spaces with application to \${K}\$-means clustering for non-{Euclidean} data},
	volume = {27},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-27/issue-1/HansonWright-inequality-in-Hilbert-spaces-with-application-to-K-means/10.3150/20-BEJ1251.full},
	doi = {10.3150/20-BEJ1251},
	abstract = {We derive a dimension-free Hanson–Wright inequality for quadratic forms of independent sub-gaussian random variables in a separable Hilbert space. Our inequality is an infinite-dimensional generalization of the classical Hanson–Wright inequality for finite-dimensional Euclidean random vectors. We illustrate an application to the generalized \$K\$-means clustering problem for non-Euclidean data. Specifically, we establish the exponential rate of convergence for a semidefinite relaxation of the generalized \$K\$-means, which together with a simple rounding algorithm imply the exact recovery of the true clustering structure.},
	number = {1},
	urldate = {2023-10-12},
	journal = {Bernoulli},
	author = {Chen, Xiaohui and Yang, Yun},
	month = feb,
	year = {2021},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {\$k\$-means, Hanson–Wright inequality, Hilbert space, semidefinite relaxation},
	pages = {586--614},
	file = {Chen_Yang_2021_Hanson–Wright inequality in Hilbert spaces with application to \$K\$-means.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Chen_Yang_2021_Hanson–Wright inequality in Hilbert spaces with application to \$K\$-means.pdf:application/pdf},
}

@article{chen_cutoff_2021,
	title = {Cutoff for {Exact} {Recovery} of {Gaussian} {Mixture} {Models}},
	volume = {67},
	issn = {0018-9448},
	url = {https://ieeexplore.ieee.org/document/9366690},
	doi = {10.1109/TIT.2021.3063155},
	abstract = {We determine the information-theoretic cutoff value on separation of cluster centers for exact recovery of cluster labels in a K-component Gaussian mixture model with equal cluster sizes. Moreover, we show that a semidefinite programming (SDP) relaxation of the K-means clustering method achieves such sharp threshold for exact recovery without assuming the symmetry of cluster centers.},
	language = {eng},
	number = {6},
	urldate = {2023-10-12},
	journal = {IEEE transactions on information theory},
	author = {Chen, Xiaohui and Yang, Yun},
	year = {2021},
	note = {Publisher: IEEE},
	keywords = {Machine learning algorithms, Partitioning algorithms, semidefinite relaxation, {\textless}italic xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"{\textgreater}K -means, Clustering algorithms, Clustering methods, exact recovery, Gaussian mixture model, Gaussian mixture models, Maximum likelihood estimation, Mixture models, optimality, sharp threshold},
	pages = {4223--4238},
	file = {Chen_Yang_2021_Cutoff for Exact Recovery of Gaussian Mixture Models.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Chen_Yang_2021_Cutoff for Exact Recovery of Gaussian Mixture Models.pdf:application/pdf},
}

@article{chen_diffusion_2021,
	title = {Diffusion {K}-means clustering on manifolds: {Provable} exact recovery via semidefinite relaxations},
	volume = {52},
	issn = {1063-5203},
	shorttitle = {Diffusion {K}-means clustering on manifolds},
	doi = {10.1016/j.acha.2020.03.002},
	abstract = {We introduce the diffusion K-means clustering method on Riemannian submanifolds, which maximizes the within-cluster connectedness based on the diffusion distance. The diffusion K-means constructs a random walk on the similarity graph with vertices as data points randomly sampled on the manifolds and edges as similarities given by a kernel that captures the local geometry of manifolds. The diffusion K-means is a multi-scale clustering tool that is suitable for data with non-linear and non-Euclidean geometric features in mixed dimensions. Given the number of clusters, we propose a polynomial-time convex relaxation algorithm via the semidefinite programming (SDP) to solve the diffusion K-means. In addition, we also propose a nuclear norm regularized SDP that is adaptive to the number of clusters. In both cases, we show that exact recovery of the SDPs for diffusion K-means can be achieved under suitable between-cluster separability and within-cluster connectedness of the submanifolds, which together quantify the hardness of the manifold clustering problem. We further propose the localized diffusion K-means by using the local adaptive bandwidth estimated from the nearest neighbors. We show that exact recovery of the localized diffusion K-means is fully adaptive to the local probability density and geometric structures of the underlying submanifolds.},
	language = {eng},
	journal = {Applied and computational harmonic analysis},
	author = {Chen, Xiaohui and Yang, Yun},
	year = {2021},
	note = {Publisher: Elsevier Inc},
	keywords = {Adaptivity, Diffusion distance, K-means, Laplace-Beltrami operator, Manifold clustering, Mixing times, Random walk on random graphs, Riemannian submanifolds, Semidefinite programming},
	pages = {303--347},
	file = {Chen_Yang_2021_Diffusion K-means clustering on manifolds.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Chen_Yang_2021_Diffusion K-means clustering on manifolds2.pdf:application/pdf},
}

@article{perakis_robust_2023,
	title = {Robust {Pricing} and {Production} with {Information} {Partitioning} and {Adaptation}},
	volume = {69},
	issn = {0025-1909},
	doi = {10.1287/mnsc.2022.4446},
	abstract = {We introduce a new distributionally robust optimization model to address a two-period, multiitem joint pricing and production problem, which can be implemented in a data-driven setting using historical demand and side information pertinent to the prediction of demands. Starting from an additive demand model, we introduce a new partitioned-moment-based ambiguity set to characterize its residuals, which also determines how the second-period demand would evolve from the first-period information in a data-driven setting. We investigate the joint pricing and production problem by proposing a cluster-adapted markdown policy and an affine recourse adaptation, which allow us to reformulate the problem as a mixed-integer linear optimization problem that we can solve to optimality using commercial solvers. We also extend our framework to ensemble methods using a set of ambiguity sets constructed from different clustering approaches. Both the numerical experiments and case study demonstrate the benefits of the cluster-adapted markdown policy and the partitioned moment-based ambiguity set in improving the mean profit over the empirical model—when applied to most out-of-sample tests.
This paper was accepted by J. George Shanthikumar, data science.
Funding:
The research of Q. Tang was supported by Nanyang Technological University [Start-Up Grant 020022-00001] and partly financed by a NUS Business School FY2018 Ph.D. Exchange Fellowship. The research of M. Sim and P. Xiong was supported by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 grant call [GrantMOE-2019-T3-1-010].
Supplemental Material:
Data and the online appendix are available at
https://doi.org/10.1287/mnsc.2022.4446
.},
	language = {eng},
	number = {3},
	journal = {Management science},
	author = {Perakis, Georgia and Sim, Melvyn and Tang, Qinshen and Xiong, Peng},
	year = {2023},
	note = {Place: Linthicum
Publisher: INFORMS},
	keywords = {Optimization, distributionally robust optimization, Adaptation, Ambiguity, Case studies, Clustering, Demand, multiitem, Prices, Pricing, Pricing policies, retail analytics},
	pages = {1398--1419},
	file = {Perakis et al_2023_Robust Pricing and Production with Information Partitioning and Adaptation.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Perakis et al_2023_Robust Pricing and Production with Information Partitioning and Adaptation.pdf:application/pdf},
}

@article{petracou_decision_2021,
	title = {Decision {Making} {Under} {Model} {Uncertainty}: {Fréchet}–{Wasserstein} {Mean} {Preferences}},
	copyright = {Copyright © 2021, INFORMS},
	shorttitle = {Decision {Making} {Under} {Model} {Uncertainty}},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2021.3961},
	doi = {10.1287/mnsc.2021.3961},
	abstract = {This paper contributes to the literature on decision making under multiple probability models by studying a class of variational preferences. These preferences are defined in terms of Fréchet mean ...},
	language = {EN},
	urldate = {2023-10-12},
	journal = {Management Science},
	author = {Petracou, Electra V. and Xepapadeas, Anastasios and Yannacopoulos, Athanasios N.},
	month = jul,
	year = {2021},
	note = {Publisher: INFORMS},
}

@incollection{peng_new_2005,
	address = {Berlin, Heidelberg},
	series = {Studies in {Fuzziness} and {Soft} {Computing}},
	title = {A {New} {Theoretical} {Framework} for {K}-{Means}-{Type} {Clustering}},
	isbn = {978-3-540-32393-8},
	url = {https://doi.org/10.1007/11362197_4},
	abstract = {One of the fundamental clustering problems is to assign n points into k clusters based on the minimal sum-of-squares(MSSC), which is known to be NP-hard. In this paper, by using matrix arguments, we first model MSSC as a so-called 0-1 semidefinite programming (SDP). The classical K-means algorithm can be interpreted as a special heuristics for the underlying 0-1 SDP. Moreover, the 0-1 SDP model can be further approximated by the relaxed and polynomially solvable linear and semidefinite programming. This opens new avenues for solving MSSC. The 0-1 SDP model can be applied not only to MSSC, but also to other scenarios of clustering as well. In particular, we show that the recently proposed normalized k-cut and spectral clustering can also be embedded into the 0-1 SDP model in various kernel spaces.},
	language = {en},
	urldate = {2023-10-17},
	booktitle = {Foundations and {Advances} in {Data} {Mining}},
	publisher = {Springer},
	author = {Peng, J. and Xia, Y.},
	editor = {Chu, Wesley and Young Lin, Tsau},
	year = {2005},
	doi = {10.1007/11362197_4},
	keywords = {Cluster Center, Feasible Solution, Kernel Space, Linear Programming Relaxation, Spectral Cluster},
	pages = {79--96},
	file = {Peng_Xia_2005_A New Theoretical Framework for K-Means-Type Clustering.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Peng_Xia_2005_A New Theoretical Framework for K-Means-Type Clustering.pdf:application/pdf},
}

@article{peng_approximating_2007,
	title = {Approximating {K}‐means‐type {Clustering} via {Semidefinite} {Programming}},
	volume = {18},
	issn = {1052-6234},
	url = {https://epubs.siam.org/doi/abs/10.1137/050641983},
	doi = {10.1137/050641983},
	abstract = {Recent work has looked at extending the k-Means algorithm to incorporate background information in the form of instance level must-link and cannot-link constraints. We introduce two ways of specifying additional background information in the form of δ and ∊ constraints that operate on all instances but which can be interpreted as conjunctions or disjunctions of instance level constraints and hence are easy to implement. We present complexity results for the feasibility of clustering under each type of constraint individually and several types together. A key finding is that determining whether there is a feasible solution satisfying all constraints is, in general, NP-complete. Thus, an iterative algorithm such as k-Means should not try to find a feasible partitioning at each iteration. This motivates our derivation of a new version of the k-Means algorithm that minimizes the constrained vector quantization error but at each iteration does not attempt to satisfy all constraints. Using standard UCI datasets, we find that using constraints improves accuracy as others have reported, but we also show that our algorithm reduces the number of iterations until convergence. Finally, we illustrate these benefits and our new constraint types on a complex real world object identification problem using the infra-red detector on an Aibo robot.},
	number = {1},
	urldate = {2023-10-17},
	journal = {SIAM Journal on Optimization},
	author = {Peng, Jiming and Wei, Yu},
	month = jan,
	year = {2007},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {186--205},
	file = {Peng_Wei_2007_Approximating K‐means‐type Clustering via Semidefinite Programming.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Peng_Wei_2007_Approximating K‐means‐type Clustering via Semidefinite Programming.pdf:application/pdf},
}

@book{james_introduction_2013,
	address = {New York},
	edition = {1st ed. 2013, Corr. 7th printing 2017 edition},
	title = {An {Introduction} to {Statistical} {Learning}: with {Applications} in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An {Introduction} to {Statistical} {Learning}},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	language = {English},
	publisher = {Springer},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {2013},
	file = {James et al_2013_An Introduction to Statistical Learning.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/James et al_2013_An Introduction to Statistical Learning.pdf:application/pdf},
}

@article{ndaoud_sharp_2022,
	title = {Sharp optimal recovery in the two component {Gaussian} mixture model},
	volume = {50},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-50/issue-4/Sharp-optimal-recovery-in-the-two-component-Gaussian-mixture-model/10.1214/22-AOS2178.full},
	doi = {10.1214/22-AOS2178},
	language = {en},
	number = {4},
	urldate = {2024-01-08},
	journal = {The Annals of Statistics},
	author = {Ndaoud, Mohamed},
	month = aug,
	year = {2022},
	file = {Ndaoud_2022_Sharp optimal recovery in the two component Gaussian mixture model.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Ndaoud_2022_Sharp optimal recovery in the two component Gaussian mixture model2.pdf:application/pdf},
}

@misc{lu_statistical_2016,
	title = {Statistical and {Computational} {Guarantees} of {Lloyd}'s {Algorithm} and its {Variants}},
	url = {http://arxiv.org/abs/1612.02099},
	doi = {10.48550/arXiv.1612.02099},
	abstract = {Clustering is a fundamental problem in statistics and machine learning. Lloyd's algorithm, proposed in 1957, is still possibly the most widely used clustering algorithm in practice due to its simplicity and empirical performance. However, there has been little theoretical investigation on the statistical and computational guarantees of Lloyd's algorithm. This paper is an attempt to bridge this gap between practice and theory. We investigate the performance of Lloyd's algorithm on clustering sub-Gaussian mixtures. Under an appropriate initialization for labels or centers, we show that Lloyd's algorithm converges to an exponentially small clustering error after an order of \${\textbackslash}log n\$ iterations, where \$n\$ is the sample size. The error rate is shown to be minimax optimal. For the two-mixture case, we only require the initializer to be slightly better than random guess. In addition, we extend the Lloyd's algorithm and its analysis to community detection and crowdsourcing, two problems that have received a lot of attention recently in statistics and machine learning. Two variants of Lloyd's algorithm are proposed respectively for community detection and crowdsourcing. On the theoretical side, we provide statistical and computational guarantees of the two algorithms, and the results improve upon some previous signal-to-noise ratio conditions in literature for both problems. Experimental results on simulated and real data sets demonstrate competitive performance of our algorithms to the state-of-the-art methods.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Lu, Yu and Zhou, Harrison H.},
	month = dec,
	year = {2016},
	note = {arXiv:1612.02099 [cs, math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Lu_Zhou_2016_Statistical and Computational Guarantees of Lloyd's Algorithm and its Variants.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Lu_Zhou_2016_Statistical and Computational Guarantees of Lloyd's Algorithm and its Variants.pdf:application/pdf},
}

@inproceedings{zhuang_likelihood_2023,
	title = {Likelihood {Adjusted} {Semidefinite} {Programs} for {Clustering} {Heterogeneous} {Data}},
	url = {https://proceedings.mlr.press/v202/zhuang23a.html},
	abstract = {Clustering is a widely deployed unsupervised learning tool. Model-based clustering is a flexible framework to tackle data heterogeneity when the clusters have different shapes. Likelihood-based inference for mixture distributions often involves non-convex and high-dimensional objective functions, imposing difficult computational and statistical challenges. The classic expectation-maximization (EM) algorithm is a computationally thrifty iterative method that maximizes a surrogate function minorizing the log-likelihood of observed data in each iteration, which however suffers from bad local maxima even in the special case of the standard Gaussian mixture model with common isotropic covariance matrices. On the other hand, recent studies reveal that the unique global solution of a semidefinite programming (SDP) relaxed \$K\$-means achieves the information-theoretically sharp threshold for perfectly recovering the cluster labels under the standard Gaussian mixture model. In this paper, we extend the SDP approach to a general setting by integrating cluster labels as model parameters and propose an iterative likelihood adjusted SDP (iLA-SDP) method that directly maximizes the exact observed likelihood in the presence of data heterogeneity. By lifting the cluster assignment to group-specific membership matrices, iLA-SDP avoids centroids estimation – a key feature that allows exact recovery under well-separateness of centroids without being trapped by their adversarial configurations. Thus iLA-SDP is less sensitive than EM to initialization and more stable on high-dimensional data. Our numeric experiments demonstrate that iLA-SDP can achieve lower mis-clustering errors over several widely used clustering methods including \$K\$-means, SDP and EM algorithms.},
	language = {en},
	urldate = {2024-01-08},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhuang, Yubo and Chen, Xiaohui and Yang, Yun},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {43326--43346},
	file = {Zhuang et al_2023_Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Zhuang et al_2023_Likelihood Adjusted Semidefinite Programs for Clustering Heterogeneous Data.pdf:application/pdf},
}

@article{han_eigen_2023,
	title = {Eigen {Selection} in {Spectral} {Clustering}: {A} {Theory}-{Guided} {Practice}},
	volume = {118},
	issn = {0162-1459},
	shorttitle = {Eigen {Selection} in {Spectral} {Clustering}},
	url = {https://doi.org/10.1080/01621459.2021.1917418},
	doi = {10.1080/01621459.2021.1917418},
	abstract = {Based on a Gaussian mixture type model of K components, we derive eigen selection procedures that improve the usual spectral clustering algorithms in high-dimensional settings, which typically act on the top few eigenvectors of an affinity matrix (e.g., X⊤X ) derived from the data matrix X . Our selection principle formalizes two intuitions: (i) eigenvectors should be dropped when they have no clustering power; (ii) some eigenvectors corresponding to smaller spiked eigenvalues should be dropped due to estimation inaccuracy. Our selection procedures lead to new spectral clustering algorithms: ESSC for K = 2 and GESSC for K {\textgreater} 2. The newly proposed algorithms enjoy better stability and compare favorably against canonical alternatives, as demonstrated in extensive simulation and multiple real data studies. Supplementary materials for this article are available online.},
	number = {541},
	urldate = {2024-01-09},
	journal = {Journal of the American Statistical Association},
	author = {Han, Xiao and Tong, Xin and Fan, Yingying},
	month = jan,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1917418},
	keywords = {Clustering, Asymptotic expansions, Eigen selection, Eigenvalues, Eigenvectors, High dimensionality, Low-rank models},
	pages = {109--121},
	file = {Han et al_2023_Eigen Selection in Spectral Clustering.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Han et al_2023_Eigen Selection in Spectral Clustering.pdf:application/pdf},
}

@article{sun_regularized_2012,
	title = {Regularized k-means clustering of high-dimensional data and its asymptotic consistency},
	volume = {6},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-6/issue-none/Regularized-k-means-clustering-of-high-dimensional-data-and-its/10.1214/12-EJS668.full},
	doi = {10.1214/12-EJS668},
	abstract = {K-means clustering is a widely used tool for cluster analysis due to its conceptual simplicity and computational efficiency. However, its performance can be distorted when clustering high-dimensional data where the number of variables becomes relatively large and many of them may contain no information about the clustering structure. This article proposes a high-dimensional cluster analysis method via regularized k-means clustering, which can simultaneously cluster similar observations and eliminate redundant variables. The key idea is to formulate the k-means clustering in a form of regularization, with an adaptive group lasso penalty term on cluster centers. In order to optimally balance the trade-off between the clustering model fitting and sparsity, a selection criterion based on clustering stability is developed. The asymptotic estimation and selection consistency of the regularized k-means clustering with diverging dimension is established. The effectiveness of the regularized k-means clustering is also demonstrated through a variety of numerical experiments as well as applications to two gene microarray examples. The regularized clustering framework can also be extended to the general model-based clustering.},
	number = {none},
	urldate = {2024-01-10},
	journal = {Electronic Journal of Statistics},
	author = {Sun, Wei and Wang, Junhui and Fang, Yixin},
	month = jan,
	year = {2012},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {Lasso, K-means, 62H30, diverging dimension, selection consistency, stability, Variable selection},
	pages = {148--167},
	file = {Sun et al_2012_Regularized k-means clustering of high-dimensional data and its asymptotic.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Sun et al_2012_Regularized k-means clustering of high-dimensional data and its asymptotic.pdf:application/pdf},
}

@inproceedings{royer_adaptive_2017,
	title = {Adaptive {Clustering} through {Semidefinite} {Programming}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/3a15c7d0bbe60300a39f76f8a5ba6896-Abstract.html},
	abstract = {We analyze the clustering problem through a flexible probabilistic model that aims to identify an optimal partition on the sample X1,...,Xn. We perform exact clustering with high probability using a convex semidefinite estimator that interprets as a corrected, relaxed version of K-means. The estimator is analyzed through a non-asymptotic framework and showed to be optimal or near-optimal in recovering the partition. Furthermore, its performances are shown to be adaptive to the problem’s effective dimension, as well as to K the unknown number of groups in this partition. We illustrate the method’s performances in comparison to other classical clustering algorithms with numerical experiments on simulated high-dimensional data.},
	urldate = {2024-01-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Royer, Martin},
	year = {2017},
	file = {Royer_2017_Adaptive Clustering through Semidefinite Programming.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Royer_2017_Adaptive Clustering through Semidefinite Programming.pdf:application/pdf},
}

@article{balakrishnan_statistical_2017,
	title = {Statistical guarantees for the {EM} algorithm: {From} population to sample-based analysis},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Statistical guarantees for the {EM} algorithm},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-1/Statistical-guarantees-for-the-EM-algorithm--From-population-to/10.1214/16-AOS1435.full},
	doi = {10.1214/16-AOS1435},
	abstract = {The EM algorithm is a widely used tool in maximum-likelihood estimation in incomplete data problems. Existing theoretical work has focused on conditions under which the iterates or likelihood values converge, and the associated rates of convergence. Such guarantees do not distinguish whether the ultimate fixed point is a near global optimum or a bad local optimum of the sample likelihood, nor do they relate the obtained fixed point to the global optima of the idealized population likelihood (obtained in the limit of infinite data). This paper develops a theoretical framework for quantifying when and how quickly EM-type iterates converge to a small neighborhood of a given global optimum of the population likelihood. For correctly specified models, such a characterization yields rigorous guarantees on the performance of certain two-stage estimators in which a suitable initial pilot estimator is refined with iterations of the EM algorithm. Our analysis is divided into two parts: a treatment of the EM and first-order EM algorithms at the population level, followed by results that apply to these algorithms on a finite set of samples. Our conditions allow for a characterization of the region of convergence of EM-type iterates to a given population fixed point, that is, the region of the parameter space over which convergence is guaranteed to a point within a small neighborhood of the specified population fixed point. We verify our conditions and give tight characterizations of the region of convergence for three canonical problems of interest: symmetric mixture of two Gaussians, symmetric mixture of two regressions and linear regression with covariates missing completely at random.},
	number = {1},
	urldate = {2024-01-12},
	journal = {The Annals of Statistics},
	author = {Balakrishnan, Sivaraman and Wainwright, Martin J. and Yu, Bin},
	month = feb,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {60K35, 62F10, 90C30, EM algorithm, first-order EM algorithm, maximum likelihood estimation, nonconvex optimization},
	pages = {77--120},
	file = {Balakrishnan et al_2017_Statistical guarantees for the EM algorithm.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Balakrishnan et al_2017_Statistical guarantees for the EM algorithm.pdf:application/pdf},
}

@article{fraley_model-based_2002,
	title = {Model-{Based} {Clustering}, {Discriminant} {Analysis}, and {Density} {Estimation}},
	volume = {97},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214502760047131},
	doi = {10.1198/016214502760047131},
	abstract = {Cluster analysis is the automated search for groups of related observations in a dataset. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures, and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as how many clusters are there, which clustering method should be used, and how should outliers be handled. We review a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, minefield detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology and discuss recent developments in model-based clustering for non-Gaussian data, high-dimensional datasets, large datasets, and Bayesian estimation.},
	number = {458},
	urldate = {2024-01-12},
	journal = {Journal of the American Statistical Association},
	author = {Fraley, Chris and Raftery, Adrian E},
	month = jun,
	year = {2002},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1198/016214502760047131},
	keywords = {EM algorithm, Bayes factor, Breast cancer diagnosis, Cluster analysis, Gene expression microarray data, Markov chain Monte Carlo, Mixture model, Outliers, Spatial point process},
	pages = {611--631},
	file = {Fraley_Raftery_2002_Model-Based Clustering, Discriminant Analysis, and Density Estimation.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Fraley_Raftery_2002_Model-Based Clustering, Discriminant Analysis, and Density Estimation.pdf:application/pdf},
}

@inproceedings{arthur_k-means_2007,
	address = {USA},
	series = {{SODA} '07},
	title = {k-means++: the advantages of careful seeding},
	isbn = {978-0-89871-624-5},
	shorttitle = {k-means++},
	abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
	urldate = {2024-01-12},
	booktitle = {Proceedings of the eighteenth annual {ACM}-{SIAM} symposium on {Discrete} algorithms},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Arthur, David and Vassilvitskii, Sergei},
	month = jan,
	year = {2007},
	pages = {1027--1035},
	file = {Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf:/Users/jmmoon/Zotero/storage/FNAA8SEU/Arthur and Vassilvitskii - k-means++ The Advantages of Careful Seeding.pdf:application/pdf},
}

@article{sun_sdpnal_2020,
	title = {{SDPNAL}+: {A} {Matlab} software for semidefinite programming with bound constraints (version 1.0)},
	volume = {35},
	issn = {1055-6788},
	shorttitle = {{SDPNAL}+},
	url = {https://doi.org/10.1080/10556788.2019.1576176},
	doi = {10.1080/10556788.2019.1576176},
	abstract = {Sdpnal+ is a MATLAB software package that implements an augmented Lagrangian based method to solve large scale semidefinite programming problems with bound constraints. The implementation was initially based on a majorized semismooth Newton-CG augmented Lagrangian method, here we designed it within an inexact symmetric Gauss-Seidel based semi-proximal ADMM/ALM (alternating direction method of multipliers/augmented Lagrangian method) framework for the purpose of deriving simpler stopping conditions and closing the gap between the practical implementation of the algorithm and the theoretical algorithm. The basic code is written in MATLAB, but some subroutines in C language are incorporated via Mex files. We also design a convenient interface for users to input their SDP models into the solver. Numerous problems arising from combinatorial optimization and binary integer quadratic programming problems have been tested to evaluate the performance of the solver. Extensive numerical experiments conducted in [L.Q. Yang, D.F. Sun, and K.C. Toh, SDPNAL+: A majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints, Math. Program. Comput. 7 (2015), pp. 331–366] show that the proposed method is quite efficient and robust, in that it is able to solve 98.9\% of the 745 test instances of SDP problems arising from various applications to the accuracy of 10−6 in the relative KKT residual.},
	number = {1},
	urldate = {2024-01-12},
	journal = {Optimization Methods and Software},
	author = {Sun, Defeng and Toh, Kim-Chuan and Yuan, Yancheng and Zhao, Xin-Yuan},
	month = jan,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10556788.2019.1576176},
	keywords = {augmented Lagrangian, Matlab software package, Semidefinite programming, semismooth Newton-CG method},
	pages = {87--115},
	file = {Sun et al_2020_SDPNAL+.pdf:/Users/jmmoon/Library/CloudStorage/GoogleDrive-jongminm@usc.edu/My Drive/paper/Sun et al_2020_SDPNAL+2.pdf:application/pdf},
}
